{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3831ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b802a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6763d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade torch sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff01a7d7-5aa4-4374-bd35-76b6ac8bab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Document: RAG enhances LLMs by retrieving external knowledge.\n"
     ]
    }
   ],
   "source": [
    "## Question 1\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"RAG enhances LLMs by retrieving external knowledge.\",\n",
    "    \"FAISS is a library for fast similarity search.\",\n",
    "    \"Vector search improves document retrieval efficiency.\"\n",
    "]\n",
    "# Convert documents to embeddings\n",
    "embeddings = np.array([model.encode(doc) for doc in docs], dtype=np.float32)\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "# User Query\n",
    "query = \"How does RAG work?\"\n",
    "query_embedding = np.array([model.encode(query)], dtype=np.float32)\n",
    "# Retrieve the best match\n",
    "D, I = index.search(query_embedding, k=1)\n",
    "retrieved_doc = docs[I[0][0]]\n",
    "print(\"Retrieved Document:\", retrieved_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec65e05-5f31-4751-8197-6ea98d5ee83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini AI Response: Based on the provided information, Gemini AI differs from traditional AI models in two key ways:\n",
      "\n",
      "* **Multimodality:**  Gemini AI processes and integrates multiple modes of information, including text, images, and code. Traditional AI models are often specialized in a single modality, such as text or images, limiting their ability to understand and generate content across different formats.\n",
      "\n",
      "* **Long-context understanding and efficient responses:** Gemini AI is optimized for understanding and processing long contexts, enabling it to handle more complex and nuanced inputs. This contrasts with some traditional models that may struggle with longer sequences of information.  Furthermore, its focus on efficient responses suggests it's designed for quicker processing and output generation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Question 2\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set API key\n",
    "genai.configure(api_key=\"AIzaSyAKQFjesi4CoxeQp9JaRtvhcBOPzhMT1j0\")\n",
    "\n",
    "# Initialize the model\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "# Sample context (retrieved documents)\n",
    "context = \"\"\"\n",
    "Gemini AI is a multimodal model supporting text, images, and code.\n",
    "It is optimized for long-context understanding and efficient responses.\n",
    "\"\"\"\n",
    "\n",
    "# User Query\n",
    "query = \"How does Gemini AI compare to traditional AI models?\"\n",
    "\n",
    "# Generate response with context\n",
    "response = model.generate_content(f\"Use the following context to answer: {context}\\n\\n{query}\")\n",
    "\n",
    "# Print response\n",
    "print(\"Gemini AI Response:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd0ca1-27ff-4ad0-9c11-55d9186e9df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Document: Hybrid search combines dense embeddings with keyword-based retrieval.\n"
     ]
    }
   ],
   "source": [
    "## Question 3\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Sample knowledge base\n",
    "docs = [\n",
    "    \"FAISS supports fast vector similarity search.\",\n",
    "    \"Hybrid search combines dense embeddings with keyword-based retrieval.\",\n",
    "    \"Efficient vector search improves information retrieval.\"\n",
    "]\n",
    "\n",
    "# Compute dense embeddings\n",
    "embeddings = np.array([model.encode(doc) for doc in docs], dtype=np.float32)\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])  # Inner Product Search\n",
    "index.add(embeddings)\n",
    "\n",
    "# User Query\n",
    "query = \"What is hybrid search?\"\n",
    "query_embedding = np.array([model.encode(query)], dtype=np.float32)\n",
    "\n",
    "# Retrieve the best document\n",
    "D, I = index.search(query_embedding, k=1)\n",
    "retrieved_doc = docs[I[0][0]]\n",
    "\n",
    "print(\"Retrieved Document:\", retrieved_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76e3a2-84a8-40bf-92b4-93b8ea001d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  What is AI?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: AI stands for Artificial Intelligence.  It's a broad field aiming to create machines capable of performing tasks that typically require human intelligence. These tasks can include learning, problem-solving, decision-making, speech recognition, visual perception, and language translation.  Essentially, AI aims to mimic or surpass human cognitive abilities in machines.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  tell one example\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: One example of AI is a spam filter for your email.  It uses AI to analyze incoming emails and determine if they are likely to be unwanted solicitations or junk mail.  It does this by looking for patterns in the sender, subject line, and email body that are common in spam messages. This analysis and decision-making process is a form of AI, as it requires the machine to \"learn\" what constitutes spam and then apply that learning to new emails.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "## Question 4\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure Gemini API Key\n",
    "genai.configure(api_key=\"AIzaSyAKQFjesi4CoxeQp9JaRtvhcBOPzhMT1j0\")\n",
    "\n",
    "# Initialize the model (Using Gemini 1.5 Pro)\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant. Maintain context throughout the conversation.\"}\n",
    "]\n",
    "\n",
    "# Function to interact with Gemini API\n",
    "def chat_with_gemini(user_input):\n",
    "    global conversation_history\n",
    "    \n",
    "    # Append user input to conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # Generate response using Gemini API\n",
    "    response = model.generate_content(\"\\n\".join([msg[\"content\"] for msg in conversation_history]))\n",
    "    \n",
    "    # Extract AI response text\n",
    "    ai_response = response.text if hasattr(response, 'text') else response.candidates[0].content\n",
    "    \n",
    "    # Append AI response to conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "    \n",
    "    return ai_response\n",
    "\n",
    "# Example User Interaction\n",
    "print(\"Chatbot: Hello! How can I assist you today?\")\n",
    "while True:\n",
    "    user_message = input(\"You: \")\n",
    "    if user_message.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    bot_reply = chat_with_gemini(user_message)\n",
    "    print(\"Chatbot:\", bot_reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20bc51f5-cba3-4ff0-917d-7ef794892e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! Ask me anything related to AI, ML, or RAG.\n",
      "Chatbot: Machine learning is a method of data analysis that automates analytical model building.\n",
      "Chatbot: Gemini AI is a multimodal model optimized for long-context understanding and reasoning.\n",
      "Chatbot: RAG (Retrieval-Augmented Generation) combines document retrieval with text generation for improved accuracy.\n",
      "Chatbot: The provided documents do not contain information about \"DL\".\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "## Question 5\n",
    "import faiss\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configure Gemini API Key\n",
    "genai.configure(api_key=\"AIzaSyAKQFjesi4CoxeQp9JaRtvhcBOPzhMT1j0\")\n",
    "\n",
    "# Load the embedding model\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # A lightweight and effective model for text embeddings\n",
    "\n",
    "# Sample documents for retrieval\n",
    "documents = [\n",
    "    \"Machine learning is a method of data analysis that automates analytical model building.\",\n",
    "    \"FAISS is a library for efficient similarity search and clustering of dense vectors.\",\n",
    "    \"Gemini AI is a multimodal model optimized for long-context understanding and reasoning.\",\n",
    "    \"Retrieval-Augmented Generation (RAG) combines document retrieval with text generation for improved accuracy.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for the documents\n",
    "document_embeddings = np.array(embed_model.encode(documents))\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(document_embeddings)\n",
    "\n",
    "# Function to retrieve relevant documents using FAISS\n",
    "def retrieve_documents(query, top_k=2):\n",
    "    query_embedding = np.array(embed_model.encode([query]))\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    retrieved_docs = [documents[i] for i in indices[0]]\n",
    "    return \"\\n\".join(retrieved_docs)\n",
    "\n",
    "# Function to generate responses using Gemini API with retrieved context\n",
    "def generate_response(query):\n",
    "    retrieved_context = retrieve_documents(query)\n",
    "    full_prompt = f\"Use the following retrieved documents to answer the query:\\n{retrieved_context}\\n\\nQuery: {query}\"\n",
    "    \n",
    "    model = genai.GenerativeModel(\"gemini-2.5-flash\")  # Using Gemini 2.5 Flash for response generation\n",
    "    response = model.generate_content(full_prompt)\n",
    "    \n",
    "    return response.text if hasattr(response, \"text\") else response.candidates[0].content\n",
    "\n",
    "# Example User Interaction\n",
    "print(\"Chatbot: Hello! Ask me anything related to AI, ML, or RAG.\")\n",
    "while True:\n",
    "    user_query = input(\"You: \")\n",
    "    if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    bot_response = generate_response(user_query)\n",
    "    print(\"Chatbot:\", bot_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0147b1c-ab00-41e5-a8dd-4af78ff7d01b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
