{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756919c4-1aa7-4cd8-9450-24fa8dbfc56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Question 1\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyBcJ1z5FQPOgE3-sY0w9O1xQJQhrYC4AvE\")\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-pro\")\n",
    "response = model.generate_content(\"Hello!\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a906300-2067-43cd-95e5-15730e021de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2054,  2003, 17768,  1029,   102,     0],\n",
       "        [  101,  4863, 11416,  6024,  9932,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Question 2\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = [\"What is RAG?\", \"Explain Generative AI.\"]\n",
    "tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")  # ✅ Use batch tokenization\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b88c4b-e680-45c0-8fee-dbbe6da4b164",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      3\u001b[39m summarizer = pipeline(\u001b[33m\"\u001b[39m\u001b[33msummarization\u001b[39m\u001b[33m\"\u001b[39m, model=\u001b[33m\"\u001b[39m\u001b[33mfacebook/bart-large-cnn\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Using a longer input text for better summarization\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "## Question 3\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Using a longer input text for better summarization\n",
    "text = \"\"\"Python is a high-level programming language used for web development, \n",
    "          data science, automation, and artificial intelligence. It is known for its \n",
    "          simplicity and readability.\"\"\"\n",
    "\n",
    "summary = summarizer(text, max_length=50, min_length=20, do_sample=False)  \n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710cff4-713e-4a42-957a-e897d956da2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG stands for **Retrieval Augmented Generation**. It's a technique in Natural Language Processing (NLP) that combines the strengths of two powerful approaches:\n",
      "\n",
      "* **Retrieval (from a knowledge base):**  This involves fetching relevant information from an external source, like a database, a collection of documents, or even the internet.  Think of it like a librarian finding the right books for your research.\n",
      "* **Generation (using a large language model):** This uses a large language model (LLM) like GPT, to generate human-quality text.  Think of it as a writer crafting a compelling story based on the research materials.\n",
      "\n",
      "Instead of relying solely on the LLM's internal knowledge (which can be outdated, incomplete, or hallucinated), RAG augments the LLM with specific, up-to-date information retrieved from a relevant source. This leads to several benefits:\n",
      "\n",
      "**How RAG Works:**\n",
      "\n",
      "1. **Query Formulation:**  The user's question or prompt is transformed into a query that can be used to search the external knowledge base.\n",
      "2. **Retrieval:** The query is used to retrieve relevant documents or passages from the knowledge base.  Different retrieval methods can be used, such as keyword search, semantic search, or vector search.\n",
      "3. **Contextualization:** The retrieved information is added to the user's original prompt as context.  This might involve simply appending the retrieved documents or creating a more structured context.\n",
      "4. **Generation:** The LLM uses both the original prompt and the retrieved context to generate a response.  The LLM can now ground its response in factual information from the knowledge base, making it more accurate and informative.\n",
      "\n",
      "**Benefits of RAG:**\n",
      "\n",
      "* **Improved Factuality:** By grounding the LLM's responses in retrieved information, RAG significantly reduces the risk of hallucinations and ensures the output is based on verifiable facts.\n",
      "* **Enhanced Relevance:** The retrieved context ensures the LLM focuses on the specific information relevant to the user's query, resulting in more focused and relevant responses.\n",
      "* **Up-to-Date Information:** RAG allows LLMs to access and utilize the latest information from external sources, overcoming the limitations of their static training data.\n",
      "* **Adaptability and Customization:**  RAG can be adapted to various domains and tasks by using different knowledge bases and retrieval methods.  This makes it a versatile tool for a wide range of applications.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Imagine you ask, \"What are the latest advancements in cancer treatment?\"  A standard LLM might provide a general overview based on its training data (which could be outdated).  With RAG, the system would first search a medical database for recent research papers on cancer treatment.  It would then provide the LLM with these papers as context, allowing it to generate a response that incorporates the latest findings and specific details.\n",
      "\n",
      "**Challenges of RAG:**\n",
      "\n",
      "* **Retrieval Quality:** The effectiveness of RAG heavily depends on the quality of the retrieved information.  If the retrieval process fails to find relevant information, the LLM's output will be compromised.\n",
      "* **Context Window Limitations:** LLMs have limited context windows, meaning they can only process a certain amount of text at a time.  This can restrict the amount of retrieved context that can be used.\n",
      "* **Computational Cost:** Retrieving and processing information from external sources can add computational overhead.\n",
      "\n",
      "Despite these challenges, RAG is a promising approach for building more powerful and reliable NLP applications. It bridges the gap between the vast knowledge stored in external sources and the generative capabilities of LLMs, enabling them to generate more informed, accurate, and relevant responses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Question 4\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ✅ Correct: Use 'api_key' argument explicitly\n",
    "genai.configure(api_key=\"AIzaSyBcJ1z5FQPOgE3-sY0w9O1xQJQhrYC4AvE\")  \n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-pro\")\n",
    "response = model.generate_content(\"Hello, explain RAG.\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b36de-b12e-4cad-b238-161884575c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2054,  2003, 11416,  6024,  9932,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "## Question 5\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"What is Generative AI?\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")  # ✅ Added return_tensors=\"pt\"\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b5884-da69-476c-acd4-14f31d99b7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.15.0a3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
