{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef9baca-0892-40a7-a891-3a063f7d31c1",
   "metadata": {},
   "source": [
    "1.You are trying to use the T5 model to perform English-to-French translation. The model expects tokenized input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d321d78-5efb-470f-b390-aff1fb3e15d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\AgenticAI\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'avenir est brillant.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_text = \"translate English to French: The future is bright.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64159ab7-9794-49fc-8d7d-924e8e097ff5",
   "metadata": {},
   "source": [
    "2.You wrote a RAG-like prompt flow but skipped the vector retrieval step. Complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa91efd-350b-48c6-9af6-b5148ebf79e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Gemini is powerful\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "documents = [\"Gemini is powerful\", \"Vector search is fast\"]\n",
    "doc_embeddings = model.encode(documents).astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatL2(384)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "query = \"Tell me about Gemini\"\n",
    "query_embedding = model.encode([query]).astype('float32')\n",
    "\n",
    "_, indices = index.search(query_embedding, k=1)\n",
    "retrieved_doc = documents[indices[0][0]]\n",
    "\n",
    "print(\"Answer:\", retrieved_doc)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f846df9-8b48-4d9e-bf8e-c0b4aa9a2d2f",
   "metadata": {},
   "source": [
    "3.You are trying to pass plain text to a model without tokenizing. Fix it to generate correct output using Hugging Face T5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ab465-8896-493c-ba5b-e9cc9fad8eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated: L'avenir est brillant.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_text = \"translate English to French: The future is bright.\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(input_ids)\n",
    "translated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Translated:\", translated)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac7acea8-eac8-4abf-8abf-855c4ff52e54",
   "metadata": {},
   "source": [
    "4.What is the correct way to retrieve the most relevant document to a query using sentence-transformers and cosine similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a58d4ba-98e6-4e37-8979-05ff2c6cb203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\AgenticAI\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant document: FAISS is used for similarity search\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "documents = [\"AI is evolving fast\", \"FAISS is used for similarity search\", \"Gemini is a generative model\"]\n",
    "query = \"Tell me about FAISS\"\n",
    "\n",
    "doc_embeddings = model.encode(documents, convert_to_tensor=True)\n",
    "query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "cosine_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)\n",
    "top_result = int(cosine_scores.argmax())\n",
    "\n",
    "print(\"Most relevant document:\", documents[top_result])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22232d78-eb2d-4100-af61-cd015ede6786",
   "metadata": {},
   "source": [
    "5.You want to generate a small sentence using a GenAI model. Use Hugging Face pipeline to do it with a tiny model. What’s the correct code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02296e7d-a213-4b93-8103-83b1e45f7d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: AI is changing the nature of the game and our industry‭…it's also doing something that\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "gen = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "out = gen(\"AI is changing\", max_length=20, num_return_sequences=1)\n",
    "\n",
    "print(\"Output:\", out[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd8572-524c-4f8c-bdec-d8feeb70883a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
