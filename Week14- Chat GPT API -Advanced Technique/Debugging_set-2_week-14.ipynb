{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "756919c4-1aa7-4cd8-9450-24fa8dbfc56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "## Question 1\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"AIzaSyAKQFjesi4CoxeQp9JaRtvhcBOPzhMT1j0\")\n",
    "model = genai.GenerativeModel(model_name=\"gemini-2.5-flash\")\n",
    "response = model.generate_content(\"Hello!\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a906300-2067-43cd-95e5-15730e021de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2054,  2003, 17768,  1029,   102,     0],\n",
       "        [  101,  4863, 11416,  6024,  9932,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Question 2\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = [\"What is RAG?\", \"Explain Generative AI.\"]\n",
    "tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")  # ✅ Use batch tokenization\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b88c4b-e680-45c0-8fee-dbbe6da4b164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BALAJI MURUGAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: C:\\Users\\BALAJI MURUGAN\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\torchvision\\image.pyd'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "c:\\Users\\BALAJI MURUGAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.0rc2) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\BALAJI MURUGAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Python is a high-level programming language used for web development, data science, automation, and artificial intelligence. It is known for its simplicity and readability.'}]\n"
     ]
    }
   ],
   "source": [
    "## Question 3\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Using a longer input text for better summarization\n",
    "text = \"\"\"Python is a high-level programming language used for web development, \n",
    "          data science, automation, and artificial intelligence. It is known for its \n",
    "          simplicity and readability.\"\"\"\n",
    "\n",
    "summary = summarizer(text, max_length=50, min_length=20, do_sample=False)  \n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710cff4-713e-4a42-957a-e897d956da2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG stands for **Retrieval Augmented Generation**. It's a technique in Natural Language Processing (NLP) that combines the strengths of two powerful approaches:\n",
      "\n",
      "* **Retrieval (from a knowledge base):**  This involves fetching relevant information from an external source, like a database, a collection of documents, or even the internet.  Think of it like a librarian finding the right books for your research.\n",
      "* **Generation (using a large language model):** This uses a large language model (LLM) like GPT, to generate human-quality text.  Think of it as a writer crafting a compelling story based on the research materials.\n",
      "\n",
      "Instead of relying solely on the LLM's internal knowledge (which can be outdated, incomplete, or hallucinated), RAG augments the LLM with specific, up-to-date information retrieved from a relevant source. This leads to several benefits:\n",
      "\n",
      "**How RAG Works:**\n",
      "\n",
      "1. **Query Formulation:**  The user's question or prompt is transformed into a query that can be used to search the external knowledge base.\n",
      "2. **Retrieval:** The query is used to retrieve relevant documents or passages from the knowledge base.  Different retrieval methods can be used, such as keyword search, semantic search, or vector search.\n",
      "3. **Contextualization:** The retrieved information is added to the user's original prompt as context.  This might involve simply appending the retrieved documents or creating a more structured context.\n",
      "4. **Generation:** The LLM uses both the original prompt and the retrieved context to generate a response.  The LLM can now ground its response in factual information from the knowledge base, making it more accurate and informative.\n",
      "\n",
      "**Benefits of RAG:**\n",
      "\n",
      "* **Improved Factuality:** By grounding the LLM's responses in retrieved information, RAG significantly reduces the risk of hallucinations and ensures the output is based on verifiable facts.\n",
      "* **Enhanced Relevance:** The retrieved context ensures the LLM focuses on the specific information relevant to the user's query, resulting in more focused and relevant responses.\n",
      "* **Up-to-Date Information:** RAG allows LLMs to access and utilize the latest information from external sources, overcoming the limitations of their static training data.\n",
      "* **Adaptability and Customization:**  RAG can be adapted to various domains and tasks by using different knowledge bases and retrieval methods.  This makes it a versatile tool for a wide range of applications.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Imagine you ask, \"What are the latest advancements in cancer treatment?\"  A standard LLM might provide a general overview based on its training data (which could be outdated).  With RAG, the system would first search a medical database for recent research papers on cancer treatment.  It would then provide the LLM with these papers as context, allowing it to generate a response that incorporates the latest findings and specific details.\n",
      "\n",
      "**Challenges of RAG:**\n",
      "\n",
      "* **Retrieval Quality:** The effectiveness of RAG heavily depends on the quality of the retrieved information.  If the retrieval process fails to find relevant information, the LLM's output will be compromised.\n",
      "* **Context Window Limitations:** LLMs have limited context windows, meaning they can only process a certain amount of text at a time.  This can restrict the amount of retrieved context that can be used.\n",
      "* **Computational Cost:** Retrieving and processing information from external sources can add computational overhead.\n",
      "\n",
      "Despite these challenges, RAG is a promising approach for building more powerful and reliable NLP applications. It bridges the gap between the vast knowledge stored in external sources and the generative capabilities of LLMs, enabling them to generate more informed, accurate, and relevant responses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Question 4\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ✅ Correct: Use 'api_key' argument explicitly\n",
    "genai.configure(api_key=\"AIzaSyAKQFjesi4CoxeQp9JaRtvhcBOPzhMT1j0\")  \n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-2.5-flash\")\n",
    "response = model.generate_content(\"Hello, explain RAG.\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b7b36de-b12e-4cad-b238-161884575c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2054,  2003, 11416,  6024,  9932,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "## Question 5\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"What is Generative AI?\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")  # ✅ Added return_tensors=\"pt\"\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b5884-da69-476c-acd4-14f31d99b7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
